# ðŸŒº Sanskrit Document Retrieval-Augmented Generation System

### **CPU-Optimized RAG for Classical Sanskrit Text Understanding**

---

## ðŸ§­ 1. Introduction

Understanding Sanskrit literature often requires domain knowledge, contextual understanding, and the ability to interpret classical grammar. This project solves that problem using a **Retrieval-Augmented Generation (RAG)** system that:

* Reads Sanskrit documents
* Converts them into meaningful vector embeddings
* Retrieves the most relevant parts
* Uses a lightweight LLM (**Phi-3 Mini Quantized**) to generate human-readable answers

This system is specifically optimized to run on **CPU-only machines**, making it accessible for any student, intern, or researcher without needing a GPU.

---

## ðŸ—ï¸ 2. System Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              Sanskrit Documents                 â”‚
                    â”‚  (docx, txt, pdf â†’ processed into text chunks) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Text Preprocessing & Chunking   â”‚
                      â”‚ (ingest.py)                     â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Embeddings Model (HuggingFace)         â”‚
                  â”‚   â†’ Generates vector representation       â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Vector Store (ChromaDB)                  â”‚
                  â”‚ â†’ Stores and indexes embedded chunks     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚     RAG Engine (rag_engine.py)                          â”‚
           â”‚  1. Retrieve top-k relevant chunks                      â”‚
           â”‚  2. Pass them into LLM prompt                           â”‚
           â”‚  3. Generate grounded output                            â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      LLM (Phi-3 Mini Quantized GGUF)       â”‚
              â”‚ CPU inference via llama-cpp-python         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                         ðŸ“ Final Answer to User
```

---

## âš¡ 3. Key Features

### âœ” CPU-Optimized

Runs on an ordinary laptop with no GPU.

### âœ” Works with Sanskrit

Supports Sanskrit â†’ English understanding.

### âœ” Fully Modular

Components separated into configuration, ingestion, retrieval, and generation.

### âœ” Lightweight LLM (Phi-3 Mini)

Fast, accurate, and quantized for low memory usage.

### âœ” Vector Database Using ChromaDB

Efficient semantic search on large text documents.

### âœ” CLI-Based Query System

User can ask real-time questions about Sanskrit documents.

---

## ðŸ“‚ 4. Repository Structure (Expanded)

```text
RAG_Sanskrit_Himanshu_Tripathi/
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ config.py
â”‚   â”‚     â”œâ”€ Contains model paths, vector DB paths
â”‚   â”‚     â”œâ”€ Embedding model selection
â”‚   â”‚     â””â”€ Chunk size and RAG parameters
â”‚   â”‚
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚     â”œâ”€ Loads Sanskrit docx
â”‚   â”‚     â”œâ”€ Cleans formatting
â”‚   â”‚     â”œâ”€ Splits text into chunks
â”‚   â”‚     â”œâ”€ Converts to embeddings
â”‚   â”‚     â””â”€ Saves ChromaDB vector store
â”‚   â”‚
â”‚   â”œâ”€â”€ rag_engine.py
â”‚   â”‚     â”œâ”€ Defines retriever
â”‚   â”‚     â”œâ”€ Defines LLM pipeline
â”‚   â”‚     â””â”€ Full RAG chain logic
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py
â”‚   â”‚     â”œâ”€ Command-line interface
â”‚   â”‚     â”œâ”€ User input â†’ RAG query
â”‚   â”‚     â””â”€ Pretty printing of results
â”‚   â”‚
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source_docs/
â”‚   â”‚     â””â”€â”€ Rag-docs.docx
â”‚   â””â”€â”€ vector_store/
â”‚         â””â”€â”€ (Auto-generated ChromaDB files)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ phi-3-mini-4k-instruct.Q4_K_M.gguf
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ Technical_Report.pdf
â”‚
â””â”€â”€ README.md
```

---

## ðŸ§ª 5. Theory Behind the System

### 5.1 What is RAG?

**RAG = Retrieval + Generation**

Instead of relying on the LLM to â€œknowâ€ the answer, it retrieves relevant information from documents. This ensures:

* High accuracy
* Grounded answers
* No hallucination

---

### 5.2 Why Phi-3 Mini?

| Feature        | Value        |
| -------------- | ------------ |
| Params         | ~3.8B        |
| Context Length | 4K tokens    |
| Quantized Size | ~2.2GB       |
| Hardware       | CPU-friendly |
| License        | Permissive   |

This makes it a powerful yet lightweight model for classical texts.

---

### 5.3 Embedding Model Used

Uses **HuggingFace Transformers Sentence Embeddings** such as:

```
sentence-transformers/all-mpnet-base-v2
```

Works excellently on multilingual (including Sanskrit) text.

---

## âš™ï¸ 6. Installation Guide

### Step 1: Navigate to Folder

```bash
cd RAG_Sanskrit_Himanshu_Tripathi
```

### Step 2: Install Requirements

```bash
pip install -r code/requirements.txt
```

### Step 3: Download Phi-3 Mini GGUF Model

1. Visit HuggingFace
2. Search: **"Phi-3-mini-4k-instruct-q4.gguf"**
3. Download
4. Rename to:

```
phi-3-mini-4k-instruct.Q4_K_M.gguf
```

5. Move file into:

```
/models/
```

---

## ðŸƒâ€â™‚ï¸ 7. Running the System

### Step 1: Build Vector Store

```bash
python code/ingest.py
```

You must see:

```
Success! Vector Database saved...
```

---

### Step 2: Run the Query Interface

```bash
python code/main.py
```

---

### Step 3: Interact

**User â†’**

```
What did the servant bring instead of sugar?
```

**RAG System â†’**
A context-grounded answer extracted from the Sanskrit text.

**User â†’**

```
Who is Kalidasa?
```

**User â†’**

```
exit
```

---

## ðŸ“Š 8. Example Output (Illustrative)

```
User: Who is the king mentioned in the second paragraph?

Retrieved Context:
"...à¤°à¤¾à¤œà¤¾ à¤ªà¥ƒà¤¥à¥à¤µà¥€à¤ªà¤¾à¤²à¤ƒ ..."

Model Response:
The king referenced in the second section is **Prithvipala**, a ruler described
as just and devoted to dharma.
```

---

## ðŸ› ï¸ 9. Troubleshooting

### âŒ Missing LangChain Modules

```bash
python -m pip install -U langchain langchain-community langchain-core
```

### âŒ Tokenizer Version Errors

```bash
pip install "tokenizers>=0.21,<0.22"
```

### âŒ ChromaDB Permission Issues

Delete and regenerate:

```bash
rm -rf data/vector_store/*
python code/ingest.py
```

---

## ðŸ’¡ 10. Future Enhancements

* Web interface (FastAPI / Streamlit)
* OCR support for Sanskrit PDFs
* GPU acceleration option
* Support for multiple documents
* Integration with cloud storage

---

## ðŸ‘¨â€ðŸ’» 11. Intern Details

**Name:** *Himanshu Tripathi*
**Project:** AI/ML Internship â€” Sanskrit RAG System
**Institute:** Birla Institute of Technology, Noida (BIT)
**Date:** November 2025
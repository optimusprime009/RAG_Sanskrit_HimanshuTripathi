# ðŸŒº Sanskrit Document Retrieval-Augmented Generation System  
### **CPU-Optimized RAG for Classical Sanskrit Text Understanding**  
Fully Dockerized â€¢ CPU-Friendly â€¢ Supports Sanskrit + English Queries

---

## ðŸ§­ 1. Introduction

Understanding Sanskrit literature requires contextual comprehension, classical grammar knowledge, and careful interpretation. This project solves that challenge using a **Retrieval-Augmented Generation (RAG)** pipeline that:

- Reads Sanskrit documents  
- Converts them into semantic embeddings  
- Retrieves the most relevant text chunk  
- Generates accurate contextual answers using a lightweight LLM  

This system is fully optimized for **CPU-only inference**, allowing anyone to run it without needing a GPU.

---

## ðŸ—ï¸ 2. System Architecture

```

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              Sanskrit Documents                 â”‚
            â”‚  (docx, txt, pdf â†’ processed into text chunks)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Text Preprocessing & Chunking   â”‚
              â”‚ (ingest.py)                     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Embeddings Model (HuggingFace)         â”‚
          â”‚   â†’ Generates vector representation      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Vector Store (ChromaDB)                  â”‚
          â”‚ â†’ Stores and indexes embedded chunks     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     RAG Engine (rag_engine.py)                          â”‚
   â”‚  1. Retrieve top-k relevant chunks                      â”‚
   â”‚  2. Pass them into LLM prompt                           â”‚
   â”‚  3. Generate grounded answer                            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚      LLM (Phi-3 Mini Quantized GGUF)       â”‚
      â”‚ CPU inference via llama-cpp-python         â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                 ðŸ“ Final Answer to User
```

````

---

## âš¡ 3. Key Features

### âœ” CPU-Optimized  
Runs smoothly on any modern CPUâ€”no GPU needed.

### âœ” Sanskrit-Supported  
Understands queries in **English and Sanskrit**, retrieves from Sanskrit documents.

### âœ” Dockerized Environment  
Run without installing Python or dependencies.

### âœ” Modular Code Structure  
Separate modules for ingestion, retrieval, configuration, and inference.

### âœ” Lightweight Local LLM (Phi-3 Mini)  
Fast, small, accurate model for classical literature tasks.

### âœ” Persistent Vector Store (ChromaDB)  
Efficient semantic retrieval for large Sanskrit documents.

---

## ðŸ“‚ 4. Repository Structure (Expanded)

```text
RAG_Sanskrit_Himanshu_Tripathi/
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ config.py              # Model paths, DB paths, constants
â”‚   â”œâ”€â”€ ingest.py              # Build vector DB from Sanskrit documents
â”‚   â”œâ”€â”€ rag_engine.py          # Retrieval + Generation pipeline
â”‚   â”œâ”€â”€ main.py                # CLI interface
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source_docs/
â”‚   â”‚     â””â”€â”€ Rag-docs.docx    # Your Sanskrit text
â”‚   â””â”€â”€ vector_store/          # Auto-generated ChromaDB files
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ phi-3-mini-4k-instruct.Q4_K_M.gguf   # LLM (manual download)
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ Technical_Report.pdf
â”‚
â”œâ”€â”€ Dockerfile                 # Container definition
â”œâ”€â”€ docker-compose.yml         # Container orchestration
â””â”€â”€ README.md
````

---

## ðŸ§ª 5. Theory Behind the System

### 5.1 What is RAG?

RAG = **Retrieval + Generation**

Instead of relying on the LLM to â€œknow everything,â€ the system retrieves relevant parts of the document and then generates an answer *grounded* in those retrieved chunks.
This prevents hallucinations and keeps answers accurate.

---

### 5.2 Why Phi-3 Mini?

| Property       | Value        |
| -------------- | ------------ |
| Params         | ~3.8B        |
| Context Window | 4K tokens    |
| Quantized Size | ~2.2GB       |
| Hardware       | CPU-Friendly |

This makes it ideal for embedded/offline Sanskrit NLP.

---

### 5.3 Embeddings

Uses:

```
sentence-transformers/all-mpnet-base-v2
```

Provides excellent multilingual performance, including Sanskrit.

---

## âš™ï¸ 6. Native Installation Guide (Python)

### Step 1 â€” Navigate to Folder

```bash
cd RAG_Sanskrit_Himanshu_Tripathi
```

### Step 2 â€” Install Requirements

```bash
pip install -r code/requirements.txt
```

### Step 3 â€” Download the Model (Manual Step)

Go to HuggingFace â†’ Search:
**Phi-3-mini-4k-instruct-q4.gguf**

Rename file to:

```
phi-3-mini-4k-instruct.Q4_K_M.gguf
```

Place it inside:

```
/models/
```

### Step 4 â€” Build Vector Store

```bash
python code/ingest.py
```

### Step 5 â€” Run Application

```bash
python code/main.py
```

Then type your queries, for example:

```
How did the servant carry the sugar?
```

---

## ðŸ³ 7. Docker Deployment (Recommended)

This setup allows you to run the entire system without installing Python locally.

### Step 1 â€” Build Docker Image

```bash
docker-compose build
```

### Step 2 â€” Create Vector Database

```bash
docker-compose run --rm rag-app python code/ingest.py
```

### Step 3 â€” Launch Interactive RAG Application

```bash
docker-compose run --rm rag-app
```

You will see the CLI:

```
>> Enter Query (English/Sanskrit):
```

---

## ðŸ“Š 8. Example Output

```
>> Enter Query: How did the servant carry the sugar?

[response]:
The servant carried the sugar in a torn cloth. Because of the torn cloth,
the sugar leaked out along the road.

[Sanskrit Evidence]
"à¤¶à¤°à¥à¤•à¤°à¤¾à¤®à¥ à¤œà¥€à¤°à¥à¤£à¥‡ à¤µà¤¸à¥à¤¤à¥à¤°à¥‡ à¤¨à¥à¤¯à¤¸à¥à¤¯à¤¤à¤¿ à¤š à¥¤
 à¤¤à¤¸à¥à¤®à¤¾à¤¤à¥ à¤œà¥€à¤°à¥à¤£à¤µà¤¸à¥à¤¤à¥à¤°à¤¾à¤¤à¥ à¤®à¤¾à¤°à¥à¤—à¥‡ à¤à¤µ à¤¸à¤°à¥à¤µà¤¾à¤ªà¤¿ à¤¶à¤°à¥à¤•à¤°à¤¾ à¤¸à¥à¤¤à¥à¤°à¤µà¤¤à¤¿ à¥¤"
```

---

## ðŸ› ï¸ 9. Troubleshooting

### Missing LangChain Modules:

```bash
python -m pip install -U langchain langchain-community langchain-core
```

### Tokenizer Version Error:

```bash
pip install "tokenizers>=0.21,<0.22"
```

### ChromaDB Permission Errors:

```bash
rm -rf data/vector_store/*
python code/ingest.py
```

---

## ðŸ‘¨â€ðŸ’» 10. Intern Details

**Name:** *Himanshu Tripathi*
**Project:** AI/ML Internship â€” Sanskrit RAG System
**Institute:** Birla Institute of Technology, Noida
**Date:** November 2025

---